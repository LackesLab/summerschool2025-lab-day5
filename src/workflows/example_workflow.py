"""LangGraph workflow that routes chat messages through an LLM component."""

from __future__ import annotations

import asyncio
import logging
from typing import Any, Iterable, Sequence

from langchain_core.messages import AIMessage, AnyMessage, BaseMessage, HumanMessage
from langchain_core.runnables import Runnable
from langgraph.graph import END, StateGraph

from llm.openrouter import build_openrouter_chat_model

from .state import WorkflowState

logger = logging.getLogger(__name__)


class _EchoFallback:
    """Asynchronous echo-style responder used when the real LLM is unavailable."""

    is_fallback = True

    async def ainvoke(self, messages: Sequence[BaseMessage]) -> AIMessage:
        last_user_message = next(
            (msg.content for msg in reversed(messages) if isinstance(msg, HumanMessage)),
            "",
        )
        if last_user_message:
            text = (
                "Echo fallback (set OPENROUTER_API_KEY for live responses): "
                f"{last_user_message}"
            )
        else:
            text = "Echo fallback waiting for your first message."
        return AIMessage(content=text)


async def _invoke_model(llm: Runnable, messages: Iterable[AnyMessage]) -> AnyMessage:
    """Call ``ainvoke`` if available, otherwise run the synchronous ``invoke``."""
    if hasattr(llm, "ainvoke"):
        return await llm.ainvoke(messages)

    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(None, llm.invoke, messages)


def _coerce_to_ai_message(message: Any) -> AIMessage:
    """Ensure downstream code receives a proper AIMessage instance."""
    if isinstance(message, AIMessage):
        return message

    if hasattr(message, "content"):
        content = message.content  # type: ignore[attr-defined]
    else:
        content = str(message)

    return AIMessage(content=str(content))


def _try_build_default_llm() -> Runnable:
    """Return the default OpenRouter-backed LLM or fall back to echo."""
    try:
        return build_openrouter_chat_model(temperature=0.3)
    except Exception as exc:  # noqa: BLE001 - surface configuration issues gracefully
        logger.warning(
            "OpenRouter LLM unavailable; falling back to echo responder.",
            exc_info=exc,
        )
        return _EchoFallback()


def build_chat_workflow(llm: Runnable | None = None) -> Any:
    """Compile and return the chat workflow using the provided LLM component."""

    llm_component: Runnable = llm or _try_build_default_llm()

    async def generate_reply(state: WorkflowState) -> WorkflowState:
        messages = state.get("messages", [])
        ai_message = _coerce_to_ai_message(
            await _invoke_model(llm_component, messages)
        )

        response_text = ai_message.content
        update: WorkflowState = {
            "messages": [ai_message],
            "response": response_text if isinstance(response_text, str) else str(response_text),
        }

        if getattr(llm_component, "is_fallback", False):
            update["notice"] = (
                "OPENROUTER_API_KEY not detected. Responses are generated by the "
                "local echo fallback."
            )

        return update

    graph = StateGraph(WorkflowState)
    graph.add_node("model", generate_reply)
    graph.set_entry_point("model")
    graph.add_edge("model", END)

    return graph.compile()
